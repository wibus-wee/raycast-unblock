[General]
mode = "local"
port = 3000
host = "127.0.0.1"

[AI]
default = "copilot"
temperature = 0.5 # If the temperature is not set in the specific AI service, this value will be used
max_tokens = 100 # If the max_tokens is not set in the specific AI service, this value will be used

[AI.OpenAI]
base_url = "https://api.openai.com"
api_key = "YOUR_API_KEY"
temperature = 0.5
max_tokens = 100

[AI.Gemini]
api_key = ""
temperature = 0.5
max_tokens = 100

[AI.Copilot]
default = "gpt-4"
api_key = ""
temperature = 0.5
max_tokens = 100

[Translate]
default = "deeplx"

[Translate.DeepLX]
# proxy_endpoint = ""
# access_token = ""

[Translate.AI]
default = "openai"

[Translate.LibreTranslate]
base_url = "https://libretranslate.com"
api_key = ""
type = "reserve"

# The following is for the legacy configuration
# They will be removed in the future
[Legacy]
ai_type = "OpenAI"
ai_api_key = ""
openai_base_url = "https://api.openai.com"
ai_max_tokens = 100
ai_temperature = 0.5

# Custom OpenAI Model
# You can add your own OpenAI model just like the following:
[AI.OpenAI.Models.gpt-3_5-turbo-16k-legacy]
isDefault = false
id = "gpt-3.5-turbo-16k.legacy" # if it's not provided, it will be generated from the Object key. For example, here it will be "gpt-3.5-turbo-16k-legacy"
model = "gpt-3.5-turbo-16k.legacy" # if it's not provided, it will be generated from the Object key.
name = "Legacy GPT-3.5 Turbo" # if it's not provided, it will be generated from the Object key.
